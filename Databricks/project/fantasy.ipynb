{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cdfb19e-0f91-46c3-a314-8a14dac24cd3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* https://www.youtube.com/watch?v=mXW7JHJM34k\n",
    "* https://cloud.google.com/bigquery/docs/reference/standard-sql/functions-and-operators\n",
    "* https://cloud.google.com/bigquery/docs/user-defined-functions\n",
    "\n",
    "![img](https://miniature-icon-2cc.notion.site/image/https%3A%2F%2Ffiles.cdn.thinkific.com%2Ffile_uploads%2F867924%2Fimages%2F72d%2F1b4%2F3a0%2Flogical_order.jpg?table=block&id=d1d63fe0-8ce5-4c6b-a6f8-3c30446364a1&spaceId=1909c126-f8a8-40ce-ba59-10d834889388&width=1400&userId=&cache=v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8d0d7c9-26d2-4fe4-96dc-e6d977317cf8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hadoop_accessed_config.lst\nazure\nconf\npreload_class.lst\neventlogs\nlogs\nmoonknight.json\n"
     ]
    }
   ],
   "source": [
    "moonknight = {\n",
    "  \"type\": \"service_account\",\n",
    "  \"project_id\": \"moon--knight\",\n",
    "  \"private_key_id\": \"3abd7ee119aac9487df83e7b5fecabee78dda445\",\n",
    "  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvAIBADANBgkqhkiG9w0BAQEFAASCBKYwggSiAgEAAoIBAQCQpr9qCcxkJnWe\\nUaAvjJnTasoebRJl3K4EM9eOUua6LR2DAwY/R3HmARxUkkCnL6V6uOmGSFyXRgua\\nVxon2qS9j4q0IV+4a8YEghDlY8HpVZ2SO8rj4Ig1cwpWUBATKWD0ulqU5Bh2+QVn\\nJJYw/g1fvVPyHSIYHxQ4Y6/ai88RiOzw30EY5RU6pSefELoLa4gx/aGFTCSJ97ni\\nTtHMyNtZ+BSqjx9DTIjhGsjX6Wrb6hO6GU0Ry8Fz43qNfnCkLZTTlN7ADB/jiXcA\\n4oXr1sHqg4xb5FOHAnwhiBVfZX9ZvebXC8GzO6/6xonGzeb5/7cqCo69iCuurAzv\\nGsyOUscFAgMBAAECggEACBVxhA0JIZ2unArmv1GFtTV2Fy40RYIILY7yIG+MmmH0\\nEMLGAhpNiLOOMEG+IDm+7zVPQ6RenEubUWRFv21uIHPZpghpb4aCWfUk2ZTyHMae\\nmDIEPCjro8UKkllGFLQOyY5yiniWz2tKPIHkbNd9ythrTRAIvD1cZWxYwwdmKo4d\\nXiHV8HlihxoqwxB4YSgr1JQuEkc5vapm+/i68IUDbRGZbm5OYHRbGYH5Goc1drAo\\n0IEKJnsV4eIgrEqCOHNCvQQ/CrbFsKqle5Ou3jJ6iOv2HidZZpbdBmQISecS03Z5\\nUcmyKzwmerosPST8B1C9kbRZMgOTvKPPWDGeQHCMAQKBgQDHVlkO7bW6VORY6tBT\\nERr9DWqE06MtCcYC/nBVmQNtiM2rbSeQQsfVmuTsljjbNenB2gqAx5Y7oMEEjxrh\\n1/LoYis4gTFQYXd4m/eDx1MSmqLcd9nwpBh4l5chLwJ4n4fsc5OawWeGGt7FCWcy\\nxwfCla/ac3LNc8/l/DECMchRBQKBgQC5xPAV+nGVOlnAvkzz6t0bx0uqjdoImSwu\\nmT0HJCcChLsZYTXf9c7JuI7MHDPmOU1G48653tRxzR9qmygS+h5IMMIrrs8KHxtQ\\nixIMz3WwZwOZIY2CqL2n4qMlMRht9q+vXsALes5G9emWldLSyxT4kUjaN+0N+OiG\\nbcLKmiJ+AQKBgHOyRGKLycxqdVa+g0eTSAzT+p05kR4U7B7UuzKTaw/qNSVoZ2wt\\ntnreOjyvPDTPZ+uhuDLipOna9ezFep7WjiAeymMzaQH0cDlKnTCZQjgsFJbN4Wrw\\nc1ua8JbMCC0muuecdXF/C60kb2QBfypTpsdjxfuOwnVI4MPlyjQx3MhZAoGANufY\\nNgbBj2Ohp3AXenhORamP2ab3bFcpUdSEzxmGVh441nV+4OTb648a1YT4afFBv9QR\\nA6qyi7gesvmMzJ8UWC3hFdwi4VD0V8fi29ptZGfuDlJ2asnI1FgN9C4glE+2+VQ9\\n2qa/VQXVtmYt25OebSEzsRuaeui4gRLr4cnQrAECgYBeFU6pyPjGFsic0tYO3uwS\\nouP7AM/9BdQvBDCEVTiOXd4hsHURf1LPIVD8Q71Vmw/WQyQeydAAelkD1NLhuedJ\\n25fBAm7t61L7YjpShtQMWzmRfuZ31CgTfR0/moopoR5gANIE16uBmVT33xFLEEnz\\nfV3GlwevWCkvQWFjwM9Xxw==\\n-----END PRIVATE KEY-----\\n\",\n",
    "  \"client_email\": \"moonknight-test-account@moon--knight.iam.gserviceaccount.com\",\n",
    "  \"client_id\": \"105304550313441617237\",\n",
    "  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
    "  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
    "  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
    "  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/moonknight-test-account%40moon--knight.iam.gserviceaccount.com\",\n",
    "  \"universe_domain\": \"googleapis.com\"\n",
    "}\n",
    "\n",
    "import os\n",
    "import json\n",
    " \n",
    "# Define file location\n",
    "file_location = \"moonknight.json\"\n",
    "\n",
    "# Write JSON content to file\n",
    "with open(file_location, \"w\") as file:\n",
    "    json.dump(moonknight, file)\n",
    "\n",
    "# Read JSON content from file\n",
    "with open(file_location, \"r\") as file:\n",
    "    content = json.load(file)\n",
    "\n",
    "# Set the environment variable\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'moonknight.json'\n",
    "\n",
    "for item in os.listdir(\".\"):\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38252f45-3299-4077-b0a2-36a480a0621b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nRequirement already satisfied: pyspark in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c338fbfd-1d96-4e96-bc4f-227d18f27a61/lib/python3.9/site-packages (3.5.1)\nRequirement already satisfied: google-cloud-bigquery in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c338fbfd-1d96-4e96-bc4f-227d18f27a61/lib/python3.9/site-packages (3.21.0)\nRequirement already satisfied: google-cloud-storage in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c338fbfd-1d96-4e96-bc4f-227d18f27a61/lib/python3.9/site-packages (2.16.0)\nRequirement already satisfied: py4j==0.10.9.7 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c338fbfd-1d96-4e96-bc4f-227d18f27a61/lib/python3.9/site-packages (from pyspark) (0.10.9.7)\nRequirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /databricks/python3/lib/python3.9/site-packages (from google-cloud-bigquery) (2.8.2)\nRequirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c338fbfd-1d96-4e96-bc4f-227d18f27a61/lib/python3.9/site-packages (from google-cloud-bigquery) (2.7.0)\nRequirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c338fbfd-1d96-4e96-bc4f-227d18f27a61/lib/python3.9/site-packages (from google-cloud-bigquery) (2.29.0)\nRequirement already satisfied: packaging>=20.0.0 in /databricks/python3/lib/python3.9/site-packages (from google-cloud-bigquery) (21.3)\nRequirement already satisfied: requests<3.0.0dev,>=2.21.0 in /databricks/python3/lib/python3.9/site-packages (from google-cloud-bigquery) (2.27.1)\nRequirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c338fbfd-1d96-4e96-bc4f-227d18f27a61/lib/python3.9/site-packages (from google-cloud-bigquery) (2.4.1)\nRequirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c338fbfd-1d96-4e96-bc4f-227d18f27a61/lib/python3.9/site-packages (from google-cloud-bigquery) (2.19.0)\nRequirement already satisfied: google-crc32c<2.0dev,>=1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c338fbfd-1d96-4e96-bc4f-227d18f27a61/lib/python3.9/site-packages (from google-cloud-storage) (1.5.0)\nRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c338fbfd-1d96-4e96-bc4f-227d18f27a61/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery) (1.63.0)\nRequirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c338fbfd-1d96-4e96-bc4f-227d18f27a61/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery) (1.23.0)\nRequirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c338fbfd-1d96-4e96-bc4f-227d18f27a61/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery) (4.25.3)\nRequirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c338fbfd-1d96-4e96-bc4f-227d18f27a61/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery) (1.62.2)\nRequirement already satisfied: grpcio<2.0dev,>=1.33.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c338fbfd-1d96-4e96-bc4f-227d18f27a61/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery) (1.63.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c338fbfd-1d96-4e96-bc4f-227d18f27a61/lib/python3.9/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery) (4.9)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c338fbfd-1d96-4e96-bc4f-227d18f27a61/lib/python3.9/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery) (5.3.3)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c338fbfd-1d96-4e96-bc4f-227d18f27a61/lib/python3.9/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery) (0.4.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.9/site-packages (from packaging>=20.0.0->google-cloud-bigquery) (3.0.4)\nRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c338fbfd-1d96-4e96-bc4f-227d18f27a61/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery) (0.6.0)\nRequirement already satisfied: six>=1.5 in /databricks/python3/lib/python3.9/site-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery) (1.16.0)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (3.3)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (2.0.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (1.26.9)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (2021.10.8)\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark google-cloud-bigquery google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "571fcef4-99eb-44fb-a4eb-3755aba54a0a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a file\n\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "client = storage.Client()\n",
    "\n",
    "# Define the bucket and file path\n",
    "bucket_name = 'moonknight-data'\n",
    "file_path = 'file.txt'\n",
    "\n",
    "# Get the bucket and file\n",
    "bucket = client.bucket(bucket_name)\n",
    "blob = bucket.blob(file_path)\n",
    "\n",
    "# Download the file contents\n",
    "file_contents = blob.download_as_text()\n",
    "\n",
    "# Print or use the file contents as needed\n",
    "print(file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1c50382-60b7-422c-92bc-a7b95812e454",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "project_id = moonknight[\"project_id\"]\n",
    "\n",
    "spark.conf.set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\")\n",
    "spark.conf.set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"moonknight.json\")\n",
    "spark.conf.set(\"spark.hadoop.fs.gs.project.id\", project_id)\n",
    "spark.conf.set(\"spark.hadoop.fs.gs.system.bucket\", bucket_name)\n",
    "spark.conf.set(\"spark.hadoop.google.cloud.auth.service.account.email\", moonknight[\"client_email\"])\n",
    "spark.conf.set(\"spark.hadoop.fs.gs.auth.service.account.private.key\", moonknight[\"private_key\"])\n",
    "spark.conf.set(\"spark.hadoop.fs.gs.auth.service.account.private.key.id\", moonknight[\"private_key_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "081d3ce4-c4d6-4e54-b2f2-091081d862f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fantasy_characters\n+---+---------+----------+-------+-----+----------+------+--------+-------------------+---------+\n| id|     name|     guild|  class|level|experience|health|is_alive|        last_active|mentor_id|\n+---+---------+----------+-------+-----+----------+------+--------+-------------------+---------+\n|  6|  Gandalf|    Istari|   Mage|   30|     10000|  90.0|    true|2023-09-20 11:00:00|     null|\n| 10|   Sauron|Dark Lords|   Mage|   40|     15000|  85.0|   false|2023-09-23 01:00:00|     null|\n| 11|  Saruman|    Istari|   Mage|   28|      8500|  88.5|   false|2023-09-20 03:00:00|        6|\n| 12|   Elrond| Rivendell|   Mage|   27|      8200|  87.5|    true|2023-09-21 04:00:00|        6|\n|  2|  Legolas|  Mirkwood| Archer|   22|      7500|100.75|    true|2023-09-21 09:00:00|     null|\n| 13|Thranduil|  Mirkwood| Archer|   26|      7900|  97.0|    true|2023-09-22 02:00:00|        2|\n| 15|  Faramir|    Gondor| Archer|   19|      5700| 118.5|    true|2023-09-20 06:00:00|        7|\n|  4|    Frodo| Shirefolk| Hobbit|   12|      2500|  50.5|   false|2023-09-23 10:00:00|        1|\n|  5|      Sam| Shirefolk| Hobbit|   13|      2750|  52.5|    true|2023-09-23 10:05:00|        1|\n|  8|    Merry| Shirefolk| Hobbit|   11|      2200|  48.5|    true|2023-09-22 10:00:00|        4|\n|  9|   Pippin| Shirefolk| Hobbit|   11|      2100|  47.5|    true|2023-09-22 10:01:00|        4|\n|  1|  Aragorn|   Rangers|Warrior|   25|      9000| 120.5|    true|2023-09-20 08:00:00|     null|\n|  3|    Gimli|    Erebor|Warrior|   20|      6000|130.25|    true|2023-09-22 07:00:00|     null|\n|  7|  Boromir|    Gondor|Warrior|   18|      5500| 115.5|   false|2023-09-21 12:00:00|     null|\n| 14|    Eowyn|     Rohan|Warrior|   17|      5200| 112.5|    true|2023-09-23 05:00:00|        7|\n+---+---------+----------+-------+-----+----------+------+--------+-------------------+---------+\n\nfantasy_inventory\n+---+------------+-------+--------+-----------+-------------------+-------------------+-----+\n| id|character_id|item_id|quantity|is_equipped|      purchase_date|        expiry_date|value|\n+---+------------+-------+--------+-----------+-------------------+-------------------+-----+\n|  4|           2|    101|       1|      false|2023-09-12 10:00:00|2023-10-12 10:00:00|   60|\n|  6|           3|      6|       1|      false|2023-09-16 09:00:00|               null|  200|\n|  8|           5|     10|       1|      false|2023-09-21 10:00:00|2023-10-21 10:00:00|  160|\n| 10|           6|     16|       1|      false|2023-09-23 09:00:00|2023-10-23 09:00:00|   30|\n| 12|           7|      4|       1|      false|2023-09-19 09:00:00|2023-10-19 09:00:00|   50|\n| 13|           8|      8|       1|      false|2023-09-20 10:00:00|               null|  800|\n| 17|          12|     15|       1|      false|2023-09-23 09:00:00|               null| 1500|\n| 20|          15|    121|       1|      false|2023-09-24 08:00:00|               null| 2000|\n|  1|           1|     99|       1|       true|2023-09-10 08:00:00|               null| 1500|\n|  3|           2|      2|       1|       true|2023-09-11 08:30:00|               null| 1100|\n|  5|           3|      3|       1|       true|2023-09-15 08:00:00|               null| 1300|\n|  9|           6|     11|       1|       true|2023-09-22 11:00:00|               null| 1700|\n| 11|           7|      7|       1|       true|2023-09-18 08:00:00|               null|  700|\n| 14|           9|     12|       1|       true|2023-09-21 11:00:00|               null| 1200|\n| 15|          10|     13|       1|       true|2023-09-22 12:00:00|               null| 1300|\n| 16|          11|     14|       1|       true|2023-09-23 08:00:00|               null| 1400|\n| 18|          13|     17|       1|       true|2023-09-23 10:00:00|               null| 1700|\n| 19|          14|     18|       1|       true|2023-09-23 11:00:00|               null| 1800|\n|  2|           1|      4|       2|      false|2023-09-12 09:00:00|2023-10-12 09:00:00|   50|\n|  7|           4|      9|       2|      false|2023-09-20 08:00:00|2023-10-20 08:00:00|  150|\n+---+------------+-------+--------+-----------+-------------------+-------------------+-----+\n\nfantasy_items\n+---+--------------------+---------+-----+------+---------+-------------------+\n| id|                name|item_type|power|weight|   rarity|         date_added|\n+---+--------------------+---------+-----+------+---------+-------------------+\n|  7|     Chainmail Armor|    Armor|   70|  18.5|     Rare|2023-09-07 10:00:00|\n| 20|Cloak of Invisibi...|    Armor|   78|   2.0|     Rare|2023-09-20 07:30:00|\n|  6|       Leather Armor|    Armor|   40|   8.0|   Common|2023-09-06 09:00:00|\n|  8|  Dragon Scale Armor|    Armor|   90|  20.0|Legendary|2023-09-08 11:00:00|\n|  4|      Healing Potion|   Potion|   50|   1.0|   Common|2023-09-04 08:00:00|\n|  5|         Mana Potion|   Potion|   60|   1.1|   Common|2023-09-05 08:30:00|\n| 16|      Stamina Elixir|   Potion|   30|   0.8|   Common|2023-09-16 09:00:00|\n| 15|  Resurrection Stone|   Potion|   99|   1.5|Legendary|2023-09-15 08:30:00|\n|  2|           Elven Bow|   Weapon|   85|   3.5|     Rare|2023-09-02 10:00:00|\n|  3|         Dwarven Axe|   Weapon|   90|  12.0|     Rare|2023-09-03 11:00:00|\n| 12|        Elven Quiver|   Weapon|   82|   2.2|     Rare|2023-09-12 10:00:00|\n| 14|     Earthquake Rune|   Weapon|   88|   1.0|     Rare|2023-09-14 08:00:00|\n| 19|     Frostbite Blade|   Weapon|   91|  10.0|     Rare|2023-09-19 07:00:00|\n|  9|     Fireball Scroll|   Weapon|   75|   0.5|   Common|2023-09-09 07:00:00|\n| 10|     Teleport Scroll|   Weapon|   80|   0.6|   Common|2023-09-10 07:30:00|\n| 17|       Lightning Rod|   Weapon|   76|   4.5|   Common|2023-09-17 10:00:00|\n| 18|        Steel Dagger|   Weapon|   65|   2.5|   Common|2023-09-18 11:00:00|\n|  1|           Excalibur|   Weapon|  100|  10.5|Legendary|2023-09-01 09:00:00|\n| 11|       Mithril Sword|   Weapon|   95|   9.5|Legendary|2023-09-11 09:00:00|\n| 13|     Phoenix Feather|   Weapon|  100|   0.1|Legendary|2023-09-13 11:00:00|\n+---+--------------------+---------+-----+------+---------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "dataset_id = \"fantasy\"\n",
    "\n",
    "# Create a BigQuery client\n",
    "client = bigquery.Client(project=project_id)\n",
    "\n",
    "# List all tables in the dataset\n",
    "tables = client.list_tables(dataset_id)\n",
    "\n",
    "df = {}\n",
    "\n",
    "# Print the list of tables\n",
    "for table in tables:\n",
    "        \n",
    "    try:\n",
    "        df[table.table_id] = spark.read.format(\"bigquery\") \\\n",
    "            .option(\"credentialsFile\", \"moonknight.json\") \\\n",
    "            .option(\"table\", f\"{table.dataset_id}.{table.table_id}\") \\\n",
    "            .option(\"project\", \"moon--knight\") \\\n",
    "            .option(\"parentProject\", \"moon--knight\") \\\n",
    "            .load()\n",
    "\n",
    "        # Register the DataFrame as a temporary view\n",
    "        df[table.table_id].createOrReplaceTempView(f\"{table.dataset_id}_{table.table_id}\")\n",
    "\n",
    "        print(f\"{table.dataset_id}_{table.table_id}\")\n",
    "        df[table.table_id].show()\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a06e1ec2-2877-49a8-ab2e-c7f6274e7172",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-684451933595834>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcharacters\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbigquery\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43moverwrite\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcredentialsFile\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmoonknight.json\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mproject\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mproject_id\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n",
       "\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mparentProject\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mproject_id\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n",
       "\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtable\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfantasy.mage\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n",
       "\u001B[1;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtemporaryGcsBucket\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbucket_name\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n",
       "\u001B[1;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1395\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n",
       "\u001B[1;32m   1393\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mformat\u001B[39m)\n",
       "\u001B[1;32m   1394\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[0;32m-> 1395\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1396\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m   1397\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave(path)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n",
       "\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 228\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    230\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n",
       "\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n",
       "\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n",
       "\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n",
       "\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n",
       "\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n",
       "\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
       "\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o600.save.\n",
       ": java.io.IOException: Error getting access token from metadata server at: http://169.254.169.254/computeMetadata/v1/instance/service-accounts/default/token\n",
       "\tat shaded.databricks.com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:250)\n",
       "\tat shaded.databricks.com.google.cloud.hadoop.util.CredentialFactory.getCredential(CredentialFactory.java:389)\n",
       "\tat shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getCredential(GoogleHadoopFileSystemBase.java:1622)\n",
       "\tat shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createGcsFs(GoogleHadoopFileSystemBase.java:1766)\n",
       "\tat shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1750)\n",
       "\tat shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:493)\n",
       "\tat com.databricks.common.filesystem.LokiFileSystem$.$anonfun$getLokiFS$1(LokiFileSystem.scala:64)\n",
       "\tat com.databricks.common.filesystem.Cache.getOrCompute(Cache.scala:38)\n",
       "\tat com.databricks.common.filesystem.LokiFileSystem$.getLokiFS(LokiFileSystem.scala:61)\n",
       "\tat com.databricks.common.filesystem.LokiFileSystem.initialize(LokiFileSystem.scala:87)\n",
       "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)\n",
       "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)\n",
       "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
       "\tat com.google.cloud.spark.bigquery.BigQueryWriteHelper.<init>(BigQueryWriteHelper.scala:63)\n",
       "\tat com.google.cloud.spark.bigquery.BigQueryInsertableRelation.insert(BigQueryInsertableRelation.scala:42)\n",
       "\tat com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:116)\n",
       "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:49)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:82)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:80)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:79)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:91)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:256)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:165)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:256)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$9(SQLExecution.scala:258)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:448)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:203)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:131)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:398)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:255)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:238)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:251)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:244)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:519)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:519)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:339)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:335)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:495)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:244)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:395)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:244)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:198)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:189)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:305)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:964)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:429)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:396)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:258)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: shaded.databricks.com.google.api.client.http.HttpResponseException: 404 Not Found\n",
       "GET http://169.254.169.254/computeMetadata/v1/instance/service-accounts/default/token\n",
       "<?xml version=\"1.0\" encoding=\"iso-8859-1\"?>\n",
       "<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n",
       "\t\"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n",
       "<html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"en\" lang=\"en\">\n",
       " <head>\n",
       "  <title>404 - Not Found</title>\n",
       " </head>\n",
       " <body>\n",
       "  <h1>404 - Not Found</h1>\n",
       " </body>\n",
       "</html>\n",
       "\n",
       "\tat shaded.databricks.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1113)\n",
       "\tat shaded.databricks.com.google.cloud.hadoop.util.CredentialFactory$ComputeCredentialWithRetry.executeRefreshToken(CredentialFactory.java:192)\n",
       "\tat shaded.databricks.com.google.api.client.auth.oauth2.Credential.refreshToken(Credential.java:470)\n",
       "\tat shaded.databricks.com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:247)\n",
       "\t... 65 more\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\nFile \u001B[0;32m<command-684451933595834>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcharacters\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbigquery\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43moverwrite\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcredentialsFile\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmoonknight.json\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mproject\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mproject_id\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mparentProject\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mproject_id\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtable\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfantasy.mage\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtemporaryGcsBucket\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbucket_name\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1395\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m   1393\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mformat\u001B[39m)\n\u001B[1;32m   1394\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1395\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1396\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1397\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave(path)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 228\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    230\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o600.save.\n: java.io.IOException: Error getting access token from metadata server at: http://169.254.169.254/computeMetadata/v1/instance/service-accounts/default/token\n\tat shaded.databricks.com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:250)\n\tat shaded.databricks.com.google.cloud.hadoop.util.CredentialFactory.getCredential(CredentialFactory.java:389)\n\tat shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getCredential(GoogleHadoopFileSystemBase.java:1622)\n\tat shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createGcsFs(GoogleHadoopFileSystemBase.java:1766)\n\tat shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1750)\n\tat shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:493)\n\tat com.databricks.common.filesystem.LokiFileSystem$.$anonfun$getLokiFS$1(LokiFileSystem.scala:64)\n\tat com.databricks.common.filesystem.Cache.getOrCompute(Cache.scala:38)\n\tat com.databricks.common.filesystem.LokiFileSystem$.getLokiFS(LokiFileSystem.scala:61)\n\tat com.databricks.common.filesystem.LokiFileSystem.initialize(LokiFileSystem.scala:87)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat com.google.cloud.spark.bigquery.BigQueryWriteHelper.<init>(BigQueryWriteHelper.scala:63)\n\tat com.google.cloud.spark.bigquery.BigQueryInsertableRelation.insert(BigQueryInsertableRelation.scala:42)\n\tat com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:116)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:49)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:82)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:80)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:79)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:91)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:256)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:165)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:256)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$9(SQLExecution.scala:258)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:448)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:203)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:131)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:398)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:255)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:238)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:251)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:244)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:519)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:519)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:339)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:335)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:495)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:244)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:395)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:244)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:198)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:189)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:305)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:964)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:429)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:396)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:258)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: shaded.databricks.com.google.api.client.http.HttpResponseException: 404 Not Found\nGET http://169.254.169.254/computeMetadata/v1/instance/service-accounts/default/token\n<?xml version=\"1.0\" encoding=\"iso-8859-1\"?>\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n\t\"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"en\" lang=\"en\">\n <head>\n  <title>404 - Not Found</title>\n </head>\n <body>\n  <h1>404 - Not Found</h1>\n </body>\n</html>\n\n\tat shaded.databricks.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1113)\n\tat shaded.databricks.com.google.cloud.hadoop.util.CredentialFactory$ComputeCredentialWithRetry.executeRefreshToken(CredentialFactory.java:192)\n\tat shaded.databricks.com.google.api.client.auth.oauth2.Credential.refreshToken(Credential.java:470)\n\tat shaded.databricks.com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:247)\n\t... 65 more\n",
       "errorSummary": "java.io.IOException: Error getting access token from metadata server at: http://169.254.169.254/computeMetadata/v1/instance/service-accounts/default/token",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"characters\"].write.format(\"bigquery\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"credentialsFile\", \"moonknight.json\") \\\n",
    "    .option(\"project\", project_id) \\\n",
    "    .option(\"parentProject\", project_id) \\\n",
    "    .option(\"table\", \"fantasy.mage\") \\\n",
    "    .option(\"temporaryGcsBucket\", bucket_name) \\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7c349eb-486e-487c-8d85-bf1f9e5d8a29",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n| class|         avg_level|\n+------+------------------+\n|  Mage|              28.5|\n|Archer|22.333333333333332|\n+------+------------------+\n\n+------+------------------+\n| class|        sqrt_level|\n+------+------------------+\n|  Mage| 5.477225575051661|\n|  Mage| 5.196152422706632|\n|Archer|5.0990195135927845|\n|Hobbit|3.4641016151377544|\n|Hobbit| 3.605551275463989|\n+------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT class, AVG(level) AS avg_level\n",
    "    FROM fantasy_characters\n",
    "    WHERE is_alive = TRUE\n",
    "    GROUP BY class\n",
    "    HAVING AVG(level) > 22\n",
    "    ORDER BY avg_level DESC\n",
    "    LIMIT 5\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT class, SQRT(level) AS sqrt_level\n",
    "    FROM fantasy_characters\n",
    "    WHERE is_alive = TRUE AND SQRT(level) > 5 OR class IN (\"Warrior\", \"Hobbit\")\n",
    "    LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34c6a92a-d392-468c-9765-cfb85041ce75",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n| class|         avg_level|\n+------+------------------+\n|  Mage|              28.5|\n|Archer|22.333333333333332|\n+------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "top_5 = df[\"characters\"].filter(df[\"characters\"]['is_alive'] == True) \\\n",
    "    .groupby('class').agg(F.avg('level').alias('avg_level')) \\\n",
    "    .filter(F.col('avg_level') > 22) \\\n",
    "    .orderBy('avg_level', ascending=False).limit(5)\n",
    "\n",
    "top_5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f86ffc7a-ecf5-4a46-9276-b24cd456d6dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+-----+-----+----------+------+--------+-------------------+---------+-----+\n| id|   name|     guild|class|level|experience|health|is_alive|        last_active|mentor_id|  exp|\n+---+-------+----------+-----+-----+----------+------+--------+-------------------+---------+-----+\n|  6|Gandalf|    Istari| Mage|   30|     10000|  90.0|    true|2023-09-20 11:00:00|     null|10000|\n| 10| Sauron|Dark Lords| Mage|   40|     15000|  85.0|   false|2023-09-23 01:00:00|     null|15000|\n| 11|Saruman|    Istari| Mage|   28|      8500|  88.5|   false|2023-09-20 03:00:00|        6| 8500|\n| 12| Elrond| Rivendell| Mage|   27|      8200|  87.5|    true|2023-09-21 04:00:00|        6| 8200|\n+---+-------+----------+-----+-----+----------+------+--------+-------------------+---------+-----+\n\n+---+-------+---------+-------+-----+----------+------+--------+-------------------+---------+\n| id|   name|    guild|  class|level|experience|health|is_alive|        last_active|mentor_id|\n+---+-------+---------+-------+-----+----------+------+--------+-------------------+---------+\n|  1|Aragorn|  Rangers|Warrior|   25|      9000| 120.5|    true|2023-09-20 08:00:00|     null|\n|  7|Boromir|   Gondor|Warrior|   18|      5500| 115.5|   false|2023-09-21 12:00:00|     null|\n| 12| Elrond|Rivendell|   Mage|   27|      8200|  87.5|    true|2023-09-21 04:00:00|        6|\n| 14|  Eowyn|    Rohan|Warrior|   17|      5200| 112.5|    true|2023-09-23 05:00:00|        7|\n+---+-------+---------+-------+-----+----------+------+--------+-------------------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS fantasy_mage\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TABLE fantasy_mage\n",
    "AS (\n",
    "    SELECT *, CAST(experience as STRING) as exp\n",
    "    FROM fantasy_characters\n",
    "    WHERE class IN (\"Mage\")\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"SELECT * FROM fantasy_mage\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT * FROM fantasy_characters WHERE class IN (\"Mage\")\n",
    "\n",
    "UNION DISTINCT\n",
    "\n",
    "SELECT * FROM fantasy_characters WHERE class NOT IN (\"Mage\")\n",
    "\n",
    "ORDER BY name\n",
    "LIMIT 4\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a269384-73ba-4589-9cdd-4f9041d8a9c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+----------+------------+--------------------+\n|sum(level)|        avg(level)|min(level)|count(level)|          class_list|\n+----------+------------------+----------+------------+--------------------+\n|       319|21.266666666666666|        11|          15|Mage,Mage,Mage,Ma...|\n+----------+------------------+----------+------------+--------------------+\n\n+-------+----------+\n|   name|experience|\n+-------+----------+\n|Gandalf|     10000|\n| Sauron|     15000|\n|Saruman|      8500|\n| Elrond|      8200|\n+-------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT SUM(level), AVG(level), MIN(level), COUNT(level), concat_ws(\",\", collect_list(class)) AS class_list\n",
    "FROM fantasy_characters\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT name, experience\n",
    "from fantasy_characters\n",
    "WHERE (\n",
    "  experience > (SELECT 2*MIN(experience) FROM fantasy_characters)\n",
    "  OR \n",
    "  experience < (SELECT MAX(experience)/2 FROM fantasy_characters)\n",
    ")\n",
    "LIMIT 4\n",
    "\"\"\").show()\n",
    "\n",
    "# spark.sql(\"\"\"\n",
    "# SELECT id as mentee, mentor_id, experience as mente_exp,\n",
    "# (\n",
    "#     SELECT experience\n",
    "#     FROM fantasy_characters as mentor_table\n",
    "#     WHERE id = mentee_table.mentor_id\n",
    "# ) AS mentor_exp\n",
    "# from fantasy_characters as mentee_table\n",
    "# LIMIT 4\n",
    "# \"\"\").show()\n",
    "\n",
    "# spark.sql(\"\"\"\n",
    "# SELECT id as mentee, mentor_id,\n",
    "# (\n",
    "#     SELECT experience\n",
    "#     FROM fantasy_characters as mentor_table\n",
    "#     WHERE id = mentee_table.mentor_id\n",
    "# ) - experience as exp_diff\n",
    "# from fantasy_characters as mentee_table\n",
    "# LIMIT 4\n",
    "# \"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14945542-4d91-4d13-91e4-81a8efe8a3f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+-----------+\n|     name|level|power_level|\n+---------+-----+-----------+\n|   Sauron|   40|      20.00|\n|  Legolas|   22|      16.50|\n|Thranduil|   26|      19.50|\n|    Frodo|   12|      18.00|\n|      Sam|   13|      19.50|\n|    Merry|   11|      16.50|\n|   Pippin|   11|      16.50|\n|  Aragorn|   25|      18.75|\n+---------+-----+-----------+\n\n+---------+-----+-----------+\n|     name|level|power_level|\n+---------+-----+-----------+\n|   Sauron|   40|      20.00|\n|Thranduil|   26|      19.50|\n|      Sam|   13|      19.50|\n|  Aragorn|   25|      18.75|\n+---------+-----+-----------+\n\n+------+-----+-----------+\n|  name|level|power_level|\n+------+-----+-----------+\n|Sauron|   40|      20.00|\n+------+-----+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * FROM \n",
    "    (SELECT name, level,\n",
    "        CASE\n",
    "            WHEN class = \"Mage\" THEN level * .5\n",
    "            WHEN class IN (\"Archer\", \"Warrior\") THEN level * .75\n",
    "            ELSE level * 1.5\n",
    "        END AS power_level \n",
    "    FROM fantasy_characters)\n",
    "WHERE power_level > 15\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "WITH power_level_table AS (\n",
    "    SELECT name, level,\n",
    "        CASE\n",
    "            WHEN class = \"Mage\" THEN level * .5\n",
    "            WHEN class IN (\"Archer\", \"Warrior\") THEN level * .75\n",
    "            ELSE level * 1.5\n",
    "        END AS power_level \n",
    "    FROM fantasy_characters\n",
    ")\n",
    "\n",
    "SELECT * FROM power_level_table WHERE power_level > 18\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "WITH \n",
    "mage_table AS (\n",
    "    SELECT *\n",
    "    FROM fantasy_characters\n",
    "    WHERE class = \"Mage\"\n",
    "),\n",
    "power_level_table AS (\n",
    "    SELECT name, level,\n",
    "        CASE\n",
    "            WHEN class = \"Mage\" THEN level * .5\n",
    "            WHEN class IN (\"Archer\", \"Warrior\") THEN level * .75\n",
    "            ELSE level * 1.5\n",
    "        END AS power_level \n",
    "    FROM mage_table\n",
    ")\n",
    "\n",
    "SELECT * FROM power_level_table WHERE power_level > 18\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7edae5ad-0948-4c91-a879-b6f8d4460b09",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+--------+------------------+-----+\n| id|     name|item_id|quantity|              name|power|\n+---+---------+-------+--------+------------------+-----+\n|  1|  Aragorn|      4|       2|    Healing Potion|   50|\n|  2|  Legolas|      2|       1|         Elven Bow|   85|\n|  3|    Gimli|      6|       1|     Leather Armor|   40|\n|  3|    Gimli|      3|       1|       Dwarven Axe|   90|\n|  4|    Frodo|      9|       2|   Fireball Scroll|   75|\n|  5|      Sam|     10|       1|   Teleport Scroll|   80|\n|  6|  Gandalf|     16|       1|    Stamina Elixir|   30|\n|  6|  Gandalf|     11|       1|     Mithril Sword|   95|\n|  7|  Boromir|      4|       1|    Healing Potion|   50|\n|  7|  Boromir|      7|       1|   Chainmail Armor|   70|\n|  8|    Merry|      8|       1|Dragon Scale Armor|   90|\n|  9|   Pippin|     12|       1|      Elven Quiver|   82|\n| 10|   Sauron|     13|       1|   Phoenix Feather|  100|\n| 11|  Saruman|     14|       1|   Earthquake Rune|   88|\n| 12|   Elrond|     15|       1|Resurrection Stone|   99|\n| 13|Thranduil|     17|       1|     Lightning Rod|   76|\n| 14|    Eowyn|     18|       1|      Steel Dagger|   65|\n+---+---------+-------+--------+------------------+-----+\n\n+---+-------+------------------+--------------------+-----+\n| id|   name|(experience / 100)|                name|power|\n+---+-------+------------------+--------------------+-----+\n|  1|Aragorn|              90.0|     Chainmail Armor|   70|\n|  1|Aragorn|              90.0|Cloak of Invisibi...|   78|\n|  1|Aragorn|              90.0|       Leather Armor|   40|\n|  1|Aragorn|              90.0|  Dragon Scale Armor|   90|\n|  1|Aragorn|              90.0|      Healing Potion|   50|\n|  1|Aragorn|              90.0|         Mana Potion|   60|\n|  1|Aragorn|              90.0|      Stamina Elixir|   30|\n|  1|Aragorn|              90.0|           Elven Bow|   85|\n|  1|Aragorn|              90.0|         Dwarven Axe|   90|\n|  1|Aragorn|              90.0|        Elven Quiver|   82|\n|  1|Aragorn|              90.0|     Earthquake Rune|   88|\n|  1|Aragorn|              90.0|     Fireball Scroll|   75|\n|  1|Aragorn|              90.0|     Teleport Scroll|   80|\n|  1|Aragorn|              90.0|       Lightning Rod|   76|\n|  1|Aragorn|              90.0|        Steel Dagger|   65|\n|  7|Boromir|              55.0|       Leather Armor|   40|\n|  7|Boromir|              55.0|      Healing Potion|   50|\n|  7|Boromir|              55.0|      Stamina Elixir|   30|\n| 12| Elrond|              82.0|     Chainmail Armor|   70|\n| 12| Elrond|              82.0|Cloak of Invisibi...|   78|\n+---+-------+------------------+--------------------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT c.id, c.name, i.item_id, i.quantity, it.name, it.power\n",
    "\n",
    "FROM fantasy_characters as c\n",
    "\n",
    "JOIN fantasy_inventory as i\n",
    "ON c.id = i.character_id \n",
    "\n",
    "JOIN fantasy_items as it\n",
    "ON i.item_id = it.id \n",
    "\n",
    "ORDER BY c.id,i.character_id \n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT c.id, c.name, c.experience / 100, it.name, it.power\n",
    "\n",
    "FROM fantasy_characters as c\n",
    "\n",
    "JOIN fantasy_items as it\n",
    "ON c.experience / 100 >= it.power\n",
    "\n",
    "ORDER BY c.name\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "693d455e-0bca-45f7-9a6a-72d919a42f71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------+-----------------+\n|  class|max(level)|count(1)|avg(length(name))|\n+-------+----------+--------+-----------------+\n|   Mage|        40|       4|              6.5|\n|Warrior|        25|       4|              6.0|\n| Hobbit|        13|       4|             4.75|\n| Archer|        26|       3|7.666666666666667|\n+-------+----------+--------+-----------------+\n\n+----------+\n|max(level)|\n+----------+\n|        40|\n+----------+\n\n+------------------+---------+---------+-----------------+\n|         min(name)|item_type|   rarity|       avg(power)|\n+------------------+---------+---------+-----------------+\n|Dragon Scale Armor|    Armor|Legendary|             90.0|\n|   Chainmail Armor|    Armor|     Rare|             74.0|\n|Resurrection Stone|   Potion|Legendary|             99.0|\n|   Fireball Scroll|   Weapon|   Common|             74.0|\n|         Excalibur|   Weapon|Legendary|98.33333333333333|\n|       Dwarven Axe|   Weapon|     Rare|             87.2|\n+------------------+---------+---------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT class, max(level), count(*), avg(length(name))\n",
    "FROM fantasy_characters\n",
    "GROUP BY class\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT max(level) FROM fantasy_characters\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT min(name), item_type, rarity, avg(power)\n",
    "FROM fantasy_items\n",
    "GROUP BY item_type, rarity\n",
    "HAVING avg(power) > 70\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bac760e-56ef-499d-bed5-8ee30a778abd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-----------------+\n|                name|item_type|        avg_power|\n+--------------------+---------+-----------------+\n|     Chainmail Armor|    Armor|             69.5|\n|Cloak of Invisibi...|    Armor|             69.5|\n|       Leather Armor|    Armor|             69.5|\n|  Dragon Scale Armor|    Armor|             69.5|\n|      Healing Potion|   Potion|            59.75|\n|         Mana Potion|   Potion|            59.75|\n|      Stamina Elixir|   Potion|            59.75|\n|  Resurrection Stone|   Potion|            59.75|\n|           Elven Bow|   Weapon|85.58333333333333|\n|         Dwarven Axe|   Weapon|85.58333333333333|\n|        Elven Quiver|   Weapon|85.58333333333333|\n|     Earthquake Rune|   Weapon|85.58333333333333|\n|     Frostbite Blade|   Weapon|85.58333333333333|\n|     Fireball Scroll|   Weapon|85.58333333333333|\n|     Teleport Scroll|   Weapon|85.58333333333333|\n|       Lightning Rod|   Weapon|85.58333333333333|\n|        Steel Dagger|   Weapon|85.58333333333333|\n|           Excalibur|   Weapon|85.58333333333333|\n|       Mithril Sword|   Weapon|85.58333333333333|\n|     Phoenix Feather|   Weapon|85.58333333333333|\n+--------------------+---------+-----------------+\n\n+--------------------+---------+-----+---------+------------------+----------+---------+---------+-------------+-------+\n|                name|item_type|power|sum_power|     percent_power|desc_power|asc_power|par_power|par_asc_power|w_power|\n+--------------------+---------+-----+---------+------------------+----------+---------+---------+-------------+-------+\n|      Stamina Elixir|   Potion|   30|     1544|1.9430051813471503|      1544|       30|      239|           30|     30|\n|       Leather Armor|    Armor|   40|     1544|2.5906735751295336|      1514|       70|      278|           40|     70|\n|      Healing Potion|   Potion|   50|     1544|3.2383419689119166|      1474|      120|      239|           80|    120|\n|         Mana Potion|   Potion|   60|     1544|3.8860103626943006|      1424|      180|      239|          140|    180|\n|        Steel Dagger|   Weapon|   65|     1544| 4.209844559585492|      1364|      245|     1027|           65|    245|\n|     Chainmail Armor|    Armor|   70|     1544| 4.533678756476684|      1299|      315|      278|          110|    315|\n|     Fireball Scroll|   Weapon|   75|     1544| 4.857512953367875|      1229|      390|     1027|          140|    390|\n|       Lightning Rod|   Weapon|   76|     1544| 4.922279792746114|      1154|      466|     1027|          216|    466|\n|Cloak of Invisibi...|    Armor|   78|     1544| 5.051813471502591|      1078|      544|      278|          188|    544|\n|     Teleport Scroll|   Weapon|   80|     1544| 5.181347150259067|      1000|      624|     1027|          296|    624|\n|        Elven Quiver|   Weapon|   82|     1544| 5.310880829015544|       920|      706|     1027|          378|    706|\n|           Elven Bow|   Weapon|   85|     1544| 5.505181347150259|       838|      791|     1027|          463|    791|\n|     Earthquake Rune|   Weapon|   88|     1544| 5.699481865284974|       753|      879|     1027|          551|    879|\n|  Dragon Scale Armor|    Armor|   90|     1544| 5.829015544041451|       665|     1059|      278|          278|    969|\n|         Dwarven Axe|   Weapon|   90|     1544| 5.829015544041451|       665|     1059|     1027|          641|   1059|\n|     Frostbite Blade|   Weapon|   91|     1544|  5.89378238341969|       485|     1150|     1027|          732|   1150|\n|       Mithril Sword|   Weapon|   95|     1544| 6.152849740932642|       394|     1245|     1027|          827|   1245|\n|  Resurrection Stone|   Potion|   99|     1544| 6.411917098445596|       299|     1344|      239|          239|   1344|\n|           Excalibur|   Weapon|  100|     1544| 6.476683937823833|       200|     1544|     1027|         1027|   1444|\n|     Phoenix Feather|   Weapon|  100|     1544| 6.476683937823833|       200|     1544|     1027|         1027|   1544|\n+--------------------+---------+-----+---------+------------------+----------+---------+---------+-------------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT name, item_type, avg(power) over(PARTITION BY item_type) as avg_power\n",
    "FROM fantasy_items\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT name, item_type, power, \n",
    "    sum(power) over() as sum_power, \n",
    "    power / sum(power) over() * 100 as percent_power,\n",
    "    sum(power) over(order by power desc) as desc_power,\n",
    "    sum(power) over(order by power asc) as asc_power,\n",
    "    sum(power) over(PARTITION BY item_type) as par_power,\n",
    "    sum(power) over(PARTITION BY item_type order by power asc) as par_asc_power,\n",
    "    sum(power) over(order by power asc, weight desc) as w_power\n",
    "FROM fantasy_items\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64d68cfa-28cc-4999-9a18-c61c0d72fbd0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-------+----------+----+\n|item_id|value|row_num|dense_rank|rank|\n+-------+-----+-------+----------+----+\n|     16|   30|      1|         1|   1|\n|      4|   50|      2|         2|   2|\n|      4|   50|      3|         2|   2|\n|    101|   60|      4|         3|   4|\n|      9|  150|      5|         4|   5|\n|     10|  160|      6|         5|   6|\n|      6|  200|      7|         6|   7|\n|      7|  700|      8|         7|   8|\n|      8|  800|      9|         8|   9|\n|      2| 1100|     10|         9|  10|\n|     12| 1200|     11|        10|  11|\n|      3| 1300|     12|        11|  12|\n|     13| 1300|     13|        11|  12|\n|     14| 1400|     14|        12|  14|\n|     15| 1500|     15|        13|  15|\n|     99| 1500|     16|        13|  15|\n|     11| 1700|     17|        14|  17|\n|     17| 1700|     18|        14|  17|\n|     18| 1800|     19|        15|  19|\n|    121| 2000|     20|        16|  20|\n+-------+-----+-------+----------+----+\n\n+-------+-----+----+\n|  class|level|rank|\n+-------+-----+----+\n| Archer|   26|   1|\n| Archer|   22|   2|\n| Archer|   19|   3|\n| Hobbit|   13|   1|\n| Hobbit|   12|   2|\n| Hobbit|   11|   3|\n| Hobbit|   11|   3|\n|   Mage|   40|   1|\n|   Mage|   30|   2|\n|   Mage|   28|   3|\n|   Mage|   27|   4|\n|Warrior|   25|   1|\n|Warrior|   20|   2|\n|Warrior|   18|   3|\n|Warrior|   17|   4|\n+-------+-----+----+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT item_id, value,\n",
    "    row_number() over(order by value) as `row_num`, \n",
    "    dense_rank() over(order by value) as `dense_rank`, \n",
    "    rank() over(order by value) as `rank`\n",
    "FROM fantasy_inventory\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT class, level,\n",
    "    rank() over(PARTITION BY class ORDER BY level DESC) as `rank`\n",
    "FROM fantasy_characters\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "176cfb97-6ac0-4215-9249-ef503f460775",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "fantasy (2)",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
