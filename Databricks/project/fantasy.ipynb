{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cdfb19e-0f91-46c3-a314-8a14dac24cd3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* https://www.youtube.com/watch?v=mXW7JHJM34k\n",
    "* https://cloud.google.com/bigquery/docs/reference/standard-sql/functions-and-operators\n",
    "* https://cloud.google.com/bigquery/docs/user-defined-functions\n",
    "\n",
    "![img](https://miniature-icon-2cc.notion.site/image/https%3A%2F%2Ffiles.cdn.thinkific.com%2Ffile_uploads%2F867924%2Fimages%2F72d%2F1b4%2F3a0%2Flogical_order.jpg?table=block&id=d1d63fe0-8ce5-4c6b-a6f8-3c30446364a1&spaceId=1909c126-f8a8-40ce-ba59-10d834889388&width=1400&userId=&cache=v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8d0d7c9-26d2-4fe4-96dc-e6d977317cf8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hadoop_accessed_config.lst\n",
      "azure\n",
      "conf\n",
      "preload_class.lst\n",
      "eventlogs\n",
      "logs\n",
      "moonknight.json\n"
     ]
    }
   ],
   "source": [
    "moonknight = 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\n",
    "\n",
    "import os\n",
    "import json\n",
    " \n",
    "# Define file location\n",
    "file_location = \"moonknight.json\"\n",
    "\n",
    "# Write JSON content to file\n",
    "with open(file_location, \"w\") as file:\n",
    "    json.dump(moonknight, file)\n",
    "\n",
    "# Read JSON content from file\n",
    "with open(file_location, \"r\") as file:\n",
    "    content = json.load(file)\n",
    "\n",
    "# Set the environment variable\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'moonknight.json'\n",
    "\n",
    "for item in os.listdir(\".\"):\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38252f45-3299-4077-b0a2-36a480a0621b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\n",
      "Requirement already satisfied: pyspark in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c338fbfd-1d96-4e96-bc4f-227d18f27a61/lib/python3.9/site-packages (3.5.1)\n",
      "Requirement already satisfied: google-cloud-bigquery in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c338fbfd-1d96-4e96-bc4f-227d18f27a61/lib/python3.9/site-packages (3.21.0)\n",
      "Requirement already satisfied: google-cloud-storage in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c338fbfd-1d96-4e96-bc4f-227d18f27a61/lib/python3.9/site-packages (2.16.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c338fbfd-1d96-4e96-bc4f-227d18f27a61/lib/python3.9/site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /databricks/python3/lib/python3.9/site-packages (from google-cloud-bigquery) (2.8.2)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c338fbfd-1d96-4e96-bc4f-227d18f27a61/lib/python3.9/site-packages (from google-cloud-bigquery) (2.7.0)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c338fbfd-1d96-4e96-bc4f-227d18f27a61/lib/python3.9/site-packages (from google-cloud-bigquery) (2.29.0)\n",
      "Requirement already satisfied: packaging>=20.0.0 in /databricks/python3/lib/python3.9/site-packages (from google-cloud-bigquery) (21.3)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.21.0 in /databricks/python3/lib/python3.9/site-packages (from google-cloud-bigquery) (2.27.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c338fbfd-1d96-4e96-bc4f-227d18f27a61/lib/python3.9/site-packages (from google-cloud-bigquery) (2.4.1)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c338fbfd-1d96-4e96-bc4f-227d18f27a61/lib/python3.9/site-packages (from google-cloud-bigquery) (2.19.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c338fbfd-1d96-4e96-bc4f-227d18f27a61/lib/python3.9/site-packages (from google-cloud-storage) (1.5.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c338fbfd-1d96-4e96-bc4f-227d18f27a61/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery) (1.63.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c338fbfd-1d96-4e96-bc4f-227d18f27a61/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery) (1.23.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c338fbfd-1d96-4e96-bc4f-227d18f27a61/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery) (4.25.3)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c338fbfd-1d96-4e96-bc4f-227d18f27a61/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery) (1.62.2)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c338fbfd-1d96-4e96-bc4f-227d18f27a61/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery) (1.63.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c338fbfd-1d96-4e96-bc4f-227d18f27a61/lib/python3.9/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c338fbfd-1d96-4e96-bc4f-227d18f27a61/lib/python3.9/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c338fbfd-1d96-4e96-bc4f-227d18f27a61/lib/python3.9/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery) (0.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.9/site-packages (from packaging>=20.0.0->google-cloud-bigquery) (3.0.4)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c338fbfd-1d96-4e96-bc4f-227d18f27a61/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery) (0.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /databricks/python3/lib/python3.9/site-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (2021.10.8)\n",
      "Python interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark google-cloud-bigquery google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "571fcef4-99eb-44fb-a4eb-3755aba54a0a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a file\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "client = storage.Client()\n",
    "\n",
    "# Define the bucket and file path\n",
    "bucket_name = 'moonknight-data'\n",
    "file_path = 'file.txt'\n",
    "\n",
    "# Get the bucket and file\n",
    "bucket = client.bucket(bucket_name)\n",
    "blob = bucket.blob(file_path)\n",
    "\n",
    "# Download the file contents\n",
    "file_contents = blob.download_as_text()\n",
    "\n",
    "# Print or use the file contents as needed\n",
    "print(file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1c50382-60b7-422c-92bc-a7b95812e454",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "project_id = moonknight[\"project_id\"]\n",
    "\n",
    "spark.conf.set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\")\n",
    "spark.conf.set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"moonknight.json\")\n",
    "spark.conf.set(\"spark.hadoop.fs.gs.project.id\", project_id)\n",
    "spark.conf.set(\"spark.hadoop.fs.gs.system.bucket\", bucket_name)\n",
    "spark.conf.set(\"spark.hadoop.google.cloud.auth.service.account.email\", moonknight[\"client_email\"])\n",
    "spark.conf.set(\"spark.hadoop.fs.gs.auth.service.account.private.key\", moonknight[\"private_key\"])\n",
    "spark.conf.set(\"spark.hadoop.fs.gs.auth.service.account.private.key.id\", moonknight[\"private_key_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "081d3ce4-c4d6-4e54-b2f2-091081d862f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fantasy_characters\n",
      "+---+---------+----------+-------+-----+----------+------+--------+-------------------+---------+\n",
      "| id|     name|     guild|  class|level|experience|health|is_alive|        last_active|mentor_id|\n",
      "+---+---------+----------+-------+-----+----------+------+--------+-------------------+---------+\n",
      "|  6|  Gandalf|    Istari|   Mage|   30|     10000|  90.0|    true|2023-09-20 11:00:00|     null|\n",
      "| 10|   Sauron|Dark Lords|   Mage|   40|     15000|  85.0|   false|2023-09-23 01:00:00|     null|\n",
      "| 11|  Saruman|    Istari|   Mage|   28|      8500|  88.5|   false|2023-09-20 03:00:00|        6|\n",
      "| 12|   Elrond| Rivendell|   Mage|   27|      8200|  87.5|    true|2023-09-21 04:00:00|        6|\n",
      "|  2|  Legolas|  Mirkwood| Archer|   22|      7500|100.75|    true|2023-09-21 09:00:00|     null|\n",
      "| 13|Thranduil|  Mirkwood| Archer|   26|      7900|  97.0|    true|2023-09-22 02:00:00|        2|\n",
      "| 15|  Faramir|    Gondor| Archer|   19|      5700| 118.5|    true|2023-09-20 06:00:00|        7|\n",
      "|  4|    Frodo| Shirefolk| Hobbit|   12|      2500|  50.5|   false|2023-09-23 10:00:00|        1|\n",
      "|  5|      Sam| Shirefolk| Hobbit|   13|      2750|  52.5|    true|2023-09-23 10:05:00|        1|\n",
      "|  8|    Merry| Shirefolk| Hobbit|   11|      2200|  48.5|    true|2023-09-22 10:00:00|        4|\n",
      "|  9|   Pippin| Shirefolk| Hobbit|   11|      2100|  47.5|    true|2023-09-22 10:01:00|        4|\n",
      "|  1|  Aragorn|   Rangers|Warrior|   25|      9000| 120.5|    true|2023-09-20 08:00:00|     null|\n",
      "|  3|    Gimli|    Erebor|Warrior|   20|      6000|130.25|    true|2023-09-22 07:00:00|     null|\n",
      "|  7|  Boromir|    Gondor|Warrior|   18|      5500| 115.5|   false|2023-09-21 12:00:00|     null|\n",
      "| 14|    Eowyn|     Rohan|Warrior|   17|      5200| 112.5|    true|2023-09-23 05:00:00|        7|\n",
      "+---+---------+----------+-------+-----+----------+------+--------+-------------------+---------+\n",
      "\n",
      "fantasy_inventory\n",
      "+---+------------+-------+--------+-----------+-------------------+-------------------+-----+\n",
      "| id|character_id|item_id|quantity|is_equipped|      purchase_date|        expiry_date|value|\n",
      "+---+------------+-------+--------+-----------+-------------------+-------------------+-----+\n",
      "|  4|           2|    101|       1|      false|2023-09-12 10:00:00|2023-10-12 10:00:00|   60|\n",
      "|  6|           3|      6|       1|      false|2023-09-16 09:00:00|               null|  200|\n",
      "|  8|           5|     10|       1|      false|2023-09-21 10:00:00|2023-10-21 10:00:00|  160|\n",
      "| 10|           6|     16|       1|      false|2023-09-23 09:00:00|2023-10-23 09:00:00|   30|\n",
      "| 12|           7|      4|       1|      false|2023-09-19 09:00:00|2023-10-19 09:00:00|   50|\n",
      "| 13|           8|      8|       1|      false|2023-09-20 10:00:00|               null|  800|\n",
      "| 17|          12|     15|       1|      false|2023-09-23 09:00:00|               null| 1500|\n",
      "| 20|          15|    121|       1|      false|2023-09-24 08:00:00|               null| 2000|\n",
      "|  1|           1|     99|       1|       true|2023-09-10 08:00:00|               null| 1500|\n",
      "|  3|           2|      2|       1|       true|2023-09-11 08:30:00|               null| 1100|\n",
      "|  5|           3|      3|       1|       true|2023-09-15 08:00:00|               null| 1300|\n",
      "|  9|           6|     11|       1|       true|2023-09-22 11:00:00|               null| 1700|\n",
      "| 11|           7|      7|       1|       true|2023-09-18 08:00:00|               null|  700|\n",
      "| 14|           9|     12|       1|       true|2023-09-21 11:00:00|               null| 1200|\n",
      "| 15|          10|     13|       1|       true|2023-09-22 12:00:00|               null| 1300|\n",
      "| 16|          11|     14|       1|       true|2023-09-23 08:00:00|               null| 1400|\n",
      "| 18|          13|     17|       1|       true|2023-09-23 10:00:00|               null| 1700|\n",
      "| 19|          14|     18|       1|       true|2023-09-23 11:00:00|               null| 1800|\n",
      "|  2|           1|      4|       2|      false|2023-09-12 09:00:00|2023-10-12 09:00:00|   50|\n",
      "|  7|           4|      9|       2|      false|2023-09-20 08:00:00|2023-10-20 08:00:00|  150|\n",
      "+---+------------+-------+--------+-----------+-------------------+-------------------+-----+\n",
      "\n",
      "fantasy_items\n",
      "+---+--------------------+---------+-----+------+---------+-------------------+\n",
      "| id|                name|item_type|power|weight|   rarity|         date_added|\n",
      "+---+--------------------+---------+-----+------+---------+-------------------+\n",
      "|  7|     Chainmail Armor|    Armor|   70|  18.5|     Rare|2023-09-07 10:00:00|\n",
      "| 20|Cloak of Invisibi...|    Armor|   78|   2.0|     Rare|2023-09-20 07:30:00|\n",
      "|  6|       Leather Armor|    Armor|   40|   8.0|   Common|2023-09-06 09:00:00|\n",
      "|  8|  Dragon Scale Armor|    Armor|   90|  20.0|Legendary|2023-09-08 11:00:00|\n",
      "|  4|      Healing Potion|   Potion|   50|   1.0|   Common|2023-09-04 08:00:00|\n",
      "|  5|         Mana Potion|   Potion|   60|   1.1|   Common|2023-09-05 08:30:00|\n",
      "| 16|      Stamina Elixir|   Potion|   30|   0.8|   Common|2023-09-16 09:00:00|\n",
      "| 15|  Resurrection Stone|   Potion|   99|   1.5|Legendary|2023-09-15 08:30:00|\n",
      "|  2|           Elven Bow|   Weapon|   85|   3.5|     Rare|2023-09-02 10:00:00|\n",
      "|  3|         Dwarven Axe|   Weapon|   90|  12.0|     Rare|2023-09-03 11:00:00|\n",
      "| 12|        Elven Quiver|   Weapon|   82|   2.2|     Rare|2023-09-12 10:00:00|\n",
      "| 14|     Earthquake Rune|   Weapon|   88|   1.0|     Rare|2023-09-14 08:00:00|\n",
      "| 19|     Frostbite Blade|   Weapon|   91|  10.0|     Rare|2023-09-19 07:00:00|\n",
      "|  9|     Fireball Scroll|   Weapon|   75|   0.5|   Common|2023-09-09 07:00:00|\n",
      "| 10|     Teleport Scroll|   Weapon|   80|   0.6|   Common|2023-09-10 07:30:00|\n",
      "| 17|       Lightning Rod|   Weapon|   76|   4.5|   Common|2023-09-17 10:00:00|\n",
      "| 18|        Steel Dagger|   Weapon|   65|   2.5|   Common|2023-09-18 11:00:00|\n",
      "|  1|           Excalibur|   Weapon|  100|  10.5|Legendary|2023-09-01 09:00:00|\n",
      "| 11|       Mithril Sword|   Weapon|   95|   9.5|Legendary|2023-09-11 09:00:00|\n",
      "| 13|     Phoenix Feather|   Weapon|  100|   0.1|Legendary|2023-09-13 11:00:00|\n",
      "+---+--------------------+---------+-----+------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "dataset_id = \"fantasy\"\n",
    "\n",
    "# Create a BigQuery client\n",
    "client = bigquery.Client(project=project_id)\n",
    "\n",
    "# List all tables in the dataset\n",
    "tables = client.list_tables(dataset_id)\n",
    "\n",
    "df = {}\n",
    "\n",
    "# Print the list of tables\n",
    "for table in tables:\n",
    "        \n",
    "    try:\n",
    "        df[table.table_id] = spark.read.format(\"bigquery\") \\\n",
    "            .option(\"credentialsFile\", \"moonknight.json\") \\\n",
    "            .option(\"table\", f\"{table.dataset_id}.{table.table_id}\") \\\n",
    "            .option(\"project\", \"moon--knight\") \\\n",
    "            .option(\"parentProject\", \"moon--knight\") \\\n",
    "            .load()\n",
    "\n",
    "        # Register the DataFrame as a temporary view\n",
    "        df[table.table_id].createOrReplaceTempView(f\"{table.dataset_id}_{table.table_id}\")\n",
    "\n",
    "        print(f\"{table.dataset_id}_{table.table_id}\")\n",
    "        df[table.table_id].show()\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a06e1ec2-2877-49a8-ab2e-c7f6274e7172",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-684451933595834>:1\u001b[0m\n",
       "\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcharacters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbigquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n",
       "\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n",
       "\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcredentialsFile\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmoonknight.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n",
       "\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mproject\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproject_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n",
       "\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparentProject\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproject_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n",
       "\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfantasy.mage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n",
       "\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemporaryGcsBucket\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbucket_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n",
       "\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m     46\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n",
       "\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "\u001b[0;32m---> 48\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m     49\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n",
       "\u001b[1;32m     50\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n",
       "\u001b[1;32m     51\u001b[0m     )\n",
       "\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1395\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n",
       "\u001b[1;32m   1393\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n",
       "\u001b[1;32m   1394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "\u001b[0;32m-> 1395\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m   1396\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "\u001b[1;32m   1397\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave(path)\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n",
       "\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n",
       "\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
       "\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n",
       "\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
       "\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n",
       "\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
       "\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "\u001b[0;32m--> 228\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
       "\u001b[1;32m    230\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n",
       "\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n",
       "\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n",
       "\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n",
       "\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
       "\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n",
       "\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n",
       "\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
       "\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
       "\n",
       "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o600.save.\n",
       ": java.io.IOException: Error getting access token from metadata server at: http://169.254.169.254/computeMetadata/v1/instance/service-accounts/default/token\n",
       "\tat shaded.databricks.com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:250)\n",
       "\tat shaded.databricks.com.google.cloud.hadoop.util.CredentialFactory.getCredential(CredentialFactory.java:389)\n",
       "\tat shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getCredential(GoogleHadoopFileSystemBase.java:1622)\n",
       "\tat shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createGcsFs(GoogleHadoopFileSystemBase.java:1766)\n",
       "\tat shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1750)\n",
       "\tat shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:493)\n",
       "\tat com.databricks.common.filesystem.LokiFileSystem$.$anonfun$getLokiFS$1(LokiFileSystem.scala:64)\n",
       "\tat com.databricks.common.filesystem.Cache.getOrCompute(Cache.scala:38)\n",
       "\tat com.databricks.common.filesystem.LokiFileSystem$.getLokiFS(LokiFileSystem.scala:61)\n",
       "\tat com.databricks.common.filesystem.LokiFileSystem.initialize(LokiFileSystem.scala:87)\n",
       "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)\n",
       "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)\n",
       "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
       "\tat com.google.cloud.spark.bigquery.BigQueryWriteHelper.<init>(BigQueryWriteHelper.scala:63)\n",
       "\tat com.google.cloud.spark.bigquery.BigQueryInsertableRelation.insert(BigQueryInsertableRelation.scala:42)\n",
       "\tat com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:116)\n",
       "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:49)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:82)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:80)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:79)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:91)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:256)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:165)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:256)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$9(SQLExecution.scala:258)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:448)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:203)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:131)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:398)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:255)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:238)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:251)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:244)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:519)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:519)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:339)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:335)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:495)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:244)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:395)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:244)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:198)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:189)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:305)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:964)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:429)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:396)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:258)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: shaded.databricks.com.google.api.client.http.HttpResponseException: 404 Not Found\n",
       "GET http://169.254.169.254/computeMetadata/v1/instance/service-accounts/default/token\n",
       "<?xml version=\"1.0\" encoding=\"iso-8859-1\"?>\n",
       "<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n",
       "\t\"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n",
       "<html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"en\" lang=\"en\">\n",
       " <head>\n",
       "  <title>404 - Not Found</title>\n",
       " </head>\n",
       " <body>\n",
       "  <h1>404 - Not Found</h1>\n",
       " </body>\n",
       "</html>\n",
       "\n",
       "\tat shaded.databricks.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1113)\n",
       "\tat shaded.databricks.com.google.cloud.hadoop.util.CredentialFactory$ComputeCredentialWithRetry.executeRefreshToken(CredentialFactory.java:192)\n",
       "\tat shaded.databricks.com.google.api.client.auth.oauth2.Credential.refreshToken(Credential.java:470)\n",
       "\tat shaded.databricks.com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:247)\n",
       "\t... 65 more\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)\nFile \u001b[0;32m<command-684451933595834>:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcharacters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbigquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcredentialsFile\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmoonknight.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mproject\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproject_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparentProject\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproject_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfantasy.mage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemporaryGcsBucket\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbucket_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n\u001b[1;32m     50\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1395\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1393\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1395\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1396\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1397\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave(path)\n\nFile \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 228\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    230\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\nFile \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\n\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o600.save.\n: java.io.IOException: Error getting access token from metadata server at: http://169.254.169.254/computeMetadata/v1/instance/service-accounts/default/token\n\tat shaded.databricks.com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:250)\n\tat shaded.databricks.com.google.cloud.hadoop.util.CredentialFactory.getCredential(CredentialFactory.java:389)\n\tat shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getCredential(GoogleHadoopFileSystemBase.java:1622)\n\tat shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createGcsFs(GoogleHadoopFileSystemBase.java:1766)\n\tat shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1750)\n\tat shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:493)\n\tat com.databricks.common.filesystem.LokiFileSystem$.$anonfun$getLokiFS$1(LokiFileSystem.scala:64)\n\tat com.databricks.common.filesystem.Cache.getOrCompute(Cache.scala:38)\n\tat com.databricks.common.filesystem.LokiFileSystem$.getLokiFS(LokiFileSystem.scala:61)\n\tat com.databricks.common.filesystem.LokiFileSystem.initialize(LokiFileSystem.scala:87)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat com.google.cloud.spark.bigquery.BigQueryWriteHelper.<init>(BigQueryWriteHelper.scala:63)\n\tat com.google.cloud.spark.bigquery.BigQueryInsertableRelation.insert(BigQueryInsertableRelation.scala:42)\n\tat com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:116)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:49)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:82)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:80)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:79)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:91)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:256)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:165)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:256)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$9(SQLExecution.scala:258)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:448)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:203)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:131)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:398)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:255)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:238)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:251)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:244)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:519)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:519)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:339)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:335)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:495)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:244)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:395)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:244)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:198)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:189)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:305)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:964)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:429)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:396)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:258)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: shaded.databricks.com.google.api.client.http.HttpResponseException: 404 Not Found\nGET http://169.254.169.254/computeMetadata/v1/instance/service-accounts/default/token\n<?xml version=\"1.0\" encoding=\"iso-8859-1\"?>\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n\t\"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"en\" lang=\"en\">\n <head>\n  <title>404 - Not Found</title>\n </head>\n <body>\n  <h1>404 - Not Found</h1>\n </body>\n</html>\n\n\tat shaded.databricks.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1113)\n\tat shaded.databricks.com.google.cloud.hadoop.util.CredentialFactory$ComputeCredentialWithRetry.executeRefreshToken(CredentialFactory.java:192)\n\tat shaded.databricks.com.google.api.client.auth.oauth2.Credential.refreshToken(Credential.java:470)\n\tat shaded.databricks.com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:247)\n\t... 65 more\n",
       "errorSummary": "java.io.IOException: Error getting access token from metadata server at: http://169.254.169.254/computeMetadata/v1/instance/service-accounts/default/token",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"characters\"].write.format(\"bigquery\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"credentialsFile\", \"moonknight.json\") \\\n",
    "    .option(\"project\", project_id) \\\n",
    "    .option(\"parentProject\", project_id) \\\n",
    "    .option(\"table\", \"fantasy.mage\") \\\n",
    "    .option(\"temporaryGcsBucket\", bucket_name) \\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7c349eb-486e-487c-8d85-bf1f9e5d8a29",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "| class|         avg_level|\n",
      "+------+------------------+\n",
      "|  Mage|              28.5|\n",
      "|Archer|22.333333333333332|\n",
      "+------+------------------+\n",
      "\n",
      "+------+------------------+\n",
      "| class|        sqrt_level|\n",
      "+------+------------------+\n",
      "|  Mage| 5.477225575051661|\n",
      "|  Mage| 5.196152422706632|\n",
      "|Archer|5.0990195135927845|\n",
      "|Hobbit|3.4641016151377544|\n",
      "|Hobbit| 3.605551275463989|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT class, AVG(level) AS avg_level\n",
    "    FROM fantasy_characters\n",
    "    WHERE is_alive = TRUE\n",
    "    GROUP BY class\n",
    "    HAVING AVG(level) > 22\n",
    "    ORDER BY avg_level DESC\n",
    "    LIMIT 5\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT class, SQRT(level) AS sqrt_level\n",
    "    FROM fantasy_characters\n",
    "    WHERE is_alive = TRUE AND SQRT(level) > 5 OR class IN (\"Warrior\", \"Hobbit\")\n",
    "    LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34c6a92a-d392-468c-9765-cfb85041ce75",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "| class|         avg_level|\n",
      "+------+------------------+\n",
      "|  Mage|              28.5|\n",
      "|Archer|22.333333333333332|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "top_5 = df[\"characters\"].filter(df[\"characters\"]['is_alive'] == True) \\\n",
    "    .groupby('class').agg(F.avg('level').alias('avg_level')) \\\n",
    "    .filter(F.col('avg_level') > 22) \\\n",
    "    .orderBy('avg_level', ascending=False).limit(5)\n",
    "\n",
    "top_5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f86ffc7a-ecf5-4a46-9276-b24cd456d6dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+-----+-----+----------+------+--------+-------------------+---------+-----+\n",
      "| id|   name|     guild|class|level|experience|health|is_alive|        last_active|mentor_id|  exp|\n",
      "+---+-------+----------+-----+-----+----------+------+--------+-------------------+---------+-----+\n",
      "|  6|Gandalf|    Istari| Mage|   30|     10000|  90.0|    true|2023-09-20 11:00:00|     null|10000|\n",
      "| 10| Sauron|Dark Lords| Mage|   40|     15000|  85.0|   false|2023-09-23 01:00:00|     null|15000|\n",
      "| 11|Saruman|    Istari| Mage|   28|      8500|  88.5|   false|2023-09-20 03:00:00|        6| 8500|\n",
      "| 12| Elrond| Rivendell| Mage|   27|      8200|  87.5|    true|2023-09-21 04:00:00|        6| 8200|\n",
      "+---+-------+----------+-----+-----+----------+------+--------+-------------------+---------+-----+\n",
      "\n",
      "+---+-------+---------+-------+-----+----------+------+--------+-------------------+---------+\n",
      "| id|   name|    guild|  class|level|experience|health|is_alive|        last_active|mentor_id|\n",
      "+---+-------+---------+-------+-----+----------+------+--------+-------------------+---------+\n",
      "|  1|Aragorn|  Rangers|Warrior|   25|      9000| 120.5|    true|2023-09-20 08:00:00|     null|\n",
      "|  7|Boromir|   Gondor|Warrior|   18|      5500| 115.5|   false|2023-09-21 12:00:00|     null|\n",
      "| 12| Elrond|Rivendell|   Mage|   27|      8200|  87.5|    true|2023-09-21 04:00:00|        6|\n",
      "| 14|  Eowyn|    Rohan|Warrior|   17|      5200| 112.5|    true|2023-09-23 05:00:00|        7|\n",
      "+---+-------+---------+-------+-----+----------+------+--------+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS fantasy_mage\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TABLE fantasy_mage\n",
    "AS (\n",
    "    SELECT *, CAST(experience as STRING) as exp\n",
    "    FROM fantasy_characters\n",
    "    WHERE class IN (\"Mage\")\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"SELECT * FROM fantasy_mage\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT * FROM fantasy_characters WHERE class IN (\"Mage\")\n",
    "\n",
    "UNION DISTINCT\n",
    "\n",
    "SELECT * FROM fantasy_characters WHERE class NOT IN (\"Mage\")\n",
    "\n",
    "ORDER BY name\n",
    "LIMIT 4\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a269384-73ba-4589-9cdd-4f9041d8a9c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+----------+------------+--------------------+\n",
      "|sum(level)|        avg(level)|min(level)|count(level)|          class_list|\n",
      "+----------+------------------+----------+------------+--------------------+\n",
      "|       319|21.266666666666666|        11|          15|Mage,Mage,Mage,Ma...|\n",
      "+----------+------------------+----------+------------+--------------------+\n",
      "\n",
      "+-------+----------+\n",
      "|   name|experience|\n",
      "+-------+----------+\n",
      "|Gandalf|     10000|\n",
      "| Sauron|     15000|\n",
      "|Saruman|      8500|\n",
      "| Elrond|      8200|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT SUM(level), AVG(level), MIN(level), COUNT(level), concat_ws(\",\", collect_list(class)) AS class_list\n",
    "FROM fantasy_characters\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT name, experience\n",
    "from fantasy_characters\n",
    "WHERE (\n",
    "  experience > (SELECT 2*MIN(experience) FROM fantasy_characters)\n",
    "  OR \n",
    "  experience < (SELECT MAX(experience)/2 FROM fantasy_characters)\n",
    ")\n",
    "LIMIT 4\n",
    "\"\"\").show()\n",
    "\n",
    "# spark.sql(\"\"\"\n",
    "# SELECT id as mentee, mentor_id, experience as mente_exp,\n",
    "# (\n",
    "#     SELECT experience\n",
    "#     FROM fantasy_characters as mentor_table\n",
    "#     WHERE id = mentee_table.mentor_id\n",
    "# ) AS mentor_exp\n",
    "# from fantasy_characters as mentee_table\n",
    "# LIMIT 4\n",
    "# \"\"\").show()\n",
    "\n",
    "# spark.sql(\"\"\"\n",
    "# SELECT id as mentee, mentor_id,\n",
    "# (\n",
    "#     SELECT experience\n",
    "#     FROM fantasy_characters as mentor_table\n",
    "#     WHERE id = mentee_table.mentor_id\n",
    "# ) - experience as exp_diff\n",
    "# from fantasy_characters as mentee_table\n",
    "# LIMIT 4\n",
    "# \"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14945542-4d91-4d13-91e4-81a8efe8a3f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+-----------+\n",
      "|     name|level|power_level|\n",
      "+---------+-----+-----------+\n",
      "|   Sauron|   40|      20.00|\n",
      "|  Legolas|   22|      16.50|\n",
      "|Thranduil|   26|      19.50|\n",
      "|    Frodo|   12|      18.00|\n",
      "|      Sam|   13|      19.50|\n",
      "|    Merry|   11|      16.50|\n",
      "|   Pippin|   11|      16.50|\n",
      "|  Aragorn|   25|      18.75|\n",
      "+---------+-----+-----------+\n",
      "\n",
      "+---------+-----+-----------+\n",
      "|     name|level|power_level|\n",
      "+---------+-----+-----------+\n",
      "|   Sauron|   40|      20.00|\n",
      "|Thranduil|   26|      19.50|\n",
      "|      Sam|   13|      19.50|\n",
      "|  Aragorn|   25|      18.75|\n",
      "+---------+-----+-----------+\n",
      "\n",
      "+------+-----+-----------+\n",
      "|  name|level|power_level|\n",
      "+------+-----+-----------+\n",
      "|Sauron|   40|      20.00|\n",
      "+------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * FROM \n",
    "    (SELECT name, level,\n",
    "        CASE\n",
    "            WHEN class = \"Mage\" THEN level * .5\n",
    "            WHEN class IN (\"Archer\", \"Warrior\") THEN level * .75\n",
    "            ELSE level * 1.5\n",
    "        END AS power_level \n",
    "    FROM fantasy_characters)\n",
    "WHERE power_level > 15\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "WITH power_level_table AS (\n",
    "    SELECT name, level,\n",
    "        CASE\n",
    "            WHEN class = \"Mage\" THEN level * .5\n",
    "            WHEN class IN (\"Archer\", \"Warrior\") THEN level * .75\n",
    "            ELSE level * 1.5\n",
    "        END AS power_level \n",
    "    FROM fantasy_characters\n",
    ")\n",
    "\n",
    "SELECT * FROM power_level_table WHERE power_level > 18\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "WITH \n",
    "mage_table AS (\n",
    "    SELECT *\n",
    "    FROM fantasy_characters\n",
    "    WHERE class = \"Mage\"\n",
    "),\n",
    "power_level_table AS (\n",
    "    SELECT name, level,\n",
    "        CASE\n",
    "            WHEN class = \"Mage\" THEN level * .5\n",
    "            WHEN class IN (\"Archer\", \"Warrior\") THEN level * .75\n",
    "            ELSE level * 1.5\n",
    "        END AS power_level \n",
    "    FROM mage_table\n",
    ")\n",
    "\n",
    "SELECT * FROM power_level_table WHERE power_level > 18\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7edae5ad-0948-4c91-a879-b6f8d4460b09",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+--------+------------------+-----+\n",
      "| id|     name|item_id|quantity|              name|power|\n",
      "+---+---------+-------+--------+------------------+-----+\n",
      "|  1|  Aragorn|      4|       2|    Healing Potion|   50|\n",
      "|  2|  Legolas|      2|       1|         Elven Bow|   85|\n",
      "|  3|    Gimli|      6|       1|     Leather Armor|   40|\n",
      "|  3|    Gimli|      3|       1|       Dwarven Axe|   90|\n",
      "|  4|    Frodo|      9|       2|   Fireball Scroll|   75|\n",
      "|  5|      Sam|     10|       1|   Teleport Scroll|   80|\n",
      "|  6|  Gandalf|     16|       1|    Stamina Elixir|   30|\n",
      "|  6|  Gandalf|     11|       1|     Mithril Sword|   95|\n",
      "|  7|  Boromir|      4|       1|    Healing Potion|   50|\n",
      "|  7|  Boromir|      7|       1|   Chainmail Armor|   70|\n",
      "|  8|    Merry|      8|       1|Dragon Scale Armor|   90|\n",
      "|  9|   Pippin|     12|       1|      Elven Quiver|   82|\n",
      "| 10|   Sauron|     13|       1|   Phoenix Feather|  100|\n",
      "| 11|  Saruman|     14|       1|   Earthquake Rune|   88|\n",
      "| 12|   Elrond|     15|       1|Resurrection Stone|   99|\n",
      "| 13|Thranduil|     17|       1|     Lightning Rod|   76|\n",
      "| 14|    Eowyn|     18|       1|      Steel Dagger|   65|\n",
      "+---+---------+-------+--------+------------------+-----+\n",
      "\n",
      "+---+-------+------------------+--------------------+-----+\n",
      "| id|   name|(experience / 100)|                name|power|\n",
      "+---+-------+------------------+--------------------+-----+\n",
      "|  1|Aragorn|              90.0|     Chainmail Armor|   70|\n",
      "|  1|Aragorn|              90.0|Cloak of Invisibi...|   78|\n",
      "|  1|Aragorn|              90.0|       Leather Armor|   40|\n",
      "|  1|Aragorn|              90.0|  Dragon Scale Armor|   90|\n",
      "|  1|Aragorn|              90.0|      Healing Potion|   50|\n",
      "|  1|Aragorn|              90.0|         Mana Potion|   60|\n",
      "|  1|Aragorn|              90.0|      Stamina Elixir|   30|\n",
      "|  1|Aragorn|              90.0|           Elven Bow|   85|\n",
      "|  1|Aragorn|              90.0|         Dwarven Axe|   90|\n",
      "|  1|Aragorn|              90.0|        Elven Quiver|   82|\n",
      "|  1|Aragorn|              90.0|     Earthquake Rune|   88|\n",
      "|  1|Aragorn|              90.0|     Fireball Scroll|   75|\n",
      "|  1|Aragorn|              90.0|     Teleport Scroll|   80|\n",
      "|  1|Aragorn|              90.0|       Lightning Rod|   76|\n",
      "|  1|Aragorn|              90.0|        Steel Dagger|   65|\n",
      "|  7|Boromir|              55.0|       Leather Armor|   40|\n",
      "|  7|Boromir|              55.0|      Healing Potion|   50|\n",
      "|  7|Boromir|              55.0|      Stamina Elixir|   30|\n",
      "| 12| Elrond|              82.0|     Chainmail Armor|   70|\n",
      "| 12| Elrond|              82.0|Cloak of Invisibi...|   78|\n",
      "+---+-------+------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT c.id, c.name, i.item_id, i.quantity, it.name, it.power\n",
    "\n",
    "FROM fantasy_characters as c\n",
    "\n",
    "JOIN fantasy_inventory as i\n",
    "ON c.id = i.character_id \n",
    "\n",
    "JOIN fantasy_items as it\n",
    "ON i.item_id = it.id \n",
    "\n",
    "ORDER BY c.id,i.character_id \n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT c.id, c.name, c.experience / 100, it.name, it.power\n",
    "\n",
    "FROM fantasy_characters as c\n",
    "\n",
    "JOIN fantasy_items as it\n",
    "ON c.experience / 100 >= it.power\n",
    "\n",
    "ORDER BY c.name\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "693d455e-0bca-45f7-9a6a-72d919a42f71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------+-----------------+\n",
      "|  class|max(level)|count(1)|avg(length(name))|\n",
      "+-------+----------+--------+-----------------+\n",
      "|   Mage|        40|       4|              6.5|\n",
      "|Warrior|        25|       4|              6.0|\n",
      "| Hobbit|        13|       4|             4.75|\n",
      "| Archer|        26|       3|7.666666666666667|\n",
      "+-------+----------+--------+-----------------+\n",
      "\n",
      "+----------+\n",
      "|max(level)|\n",
      "+----------+\n",
      "|        40|\n",
      "+----------+\n",
      "\n",
      "+------------------+---------+---------+-----------------+\n",
      "|         min(name)|item_type|   rarity|       avg(power)|\n",
      "+------------------+---------+---------+-----------------+\n",
      "|Dragon Scale Armor|    Armor|Legendary|             90.0|\n",
      "|   Chainmail Armor|    Armor|     Rare|             74.0|\n",
      "|Resurrection Stone|   Potion|Legendary|             99.0|\n",
      "|   Fireball Scroll|   Weapon|   Common|             74.0|\n",
      "|         Excalibur|   Weapon|Legendary|98.33333333333333|\n",
      "|       Dwarven Axe|   Weapon|     Rare|             87.2|\n",
      "+------------------+---------+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT class, max(level), count(*), avg(length(name))\n",
    "FROM fantasy_characters\n",
    "GROUP BY class\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT max(level) FROM fantasy_characters\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT min(name), item_type, rarity, avg(power)\n",
    "FROM fantasy_items\n",
    "GROUP BY item_type, rarity\n",
    "HAVING avg(power) > 70\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bac760e-56ef-499d-bed5-8ee30a778abd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-----------------+\n",
      "|                name|item_type|        avg_power|\n",
      "+--------------------+---------+-----------------+\n",
      "|     Chainmail Armor|    Armor|             69.5|\n",
      "|Cloak of Invisibi...|    Armor|             69.5|\n",
      "|       Leather Armor|    Armor|             69.5|\n",
      "|  Dragon Scale Armor|    Armor|             69.5|\n",
      "|      Healing Potion|   Potion|            59.75|\n",
      "|         Mana Potion|   Potion|            59.75|\n",
      "|      Stamina Elixir|   Potion|            59.75|\n",
      "|  Resurrection Stone|   Potion|            59.75|\n",
      "|           Elven Bow|   Weapon|85.58333333333333|\n",
      "|         Dwarven Axe|   Weapon|85.58333333333333|\n",
      "|        Elven Quiver|   Weapon|85.58333333333333|\n",
      "|     Earthquake Rune|   Weapon|85.58333333333333|\n",
      "|     Frostbite Blade|   Weapon|85.58333333333333|\n",
      "|     Fireball Scroll|   Weapon|85.58333333333333|\n",
      "|     Teleport Scroll|   Weapon|85.58333333333333|\n",
      "|       Lightning Rod|   Weapon|85.58333333333333|\n",
      "|        Steel Dagger|   Weapon|85.58333333333333|\n",
      "|           Excalibur|   Weapon|85.58333333333333|\n",
      "|       Mithril Sword|   Weapon|85.58333333333333|\n",
      "|     Phoenix Feather|   Weapon|85.58333333333333|\n",
      "+--------------------+---------+-----------------+\n",
      "\n",
      "+--------------------+---------+-----+---------+------------------+----------+---------+---------+-------------+-------+\n",
      "|                name|item_type|power|sum_power|     percent_power|desc_power|asc_power|par_power|par_asc_power|w_power|\n",
      "+--------------------+---------+-----+---------+------------------+----------+---------+---------+-------------+-------+\n",
      "|      Stamina Elixir|   Potion|   30|     1544|1.9430051813471503|      1544|       30|      239|           30|     30|\n",
      "|       Leather Armor|    Armor|   40|     1544|2.5906735751295336|      1514|       70|      278|           40|     70|\n",
      "|      Healing Potion|   Potion|   50|     1544|3.2383419689119166|      1474|      120|      239|           80|    120|\n",
      "|         Mana Potion|   Potion|   60|     1544|3.8860103626943006|      1424|      180|      239|          140|    180|\n",
      "|        Steel Dagger|   Weapon|   65|     1544| 4.209844559585492|      1364|      245|     1027|           65|    245|\n",
      "|     Chainmail Armor|    Armor|   70|     1544| 4.533678756476684|      1299|      315|      278|          110|    315|\n",
      "|     Fireball Scroll|   Weapon|   75|     1544| 4.857512953367875|      1229|      390|     1027|          140|    390|\n",
      "|       Lightning Rod|   Weapon|   76|     1544| 4.922279792746114|      1154|      466|     1027|          216|    466|\n",
      "|Cloak of Invisibi...|    Armor|   78|     1544| 5.051813471502591|      1078|      544|      278|          188|    544|\n",
      "|     Teleport Scroll|   Weapon|   80|     1544| 5.181347150259067|      1000|      624|     1027|          296|    624|\n",
      "|        Elven Quiver|   Weapon|   82|     1544| 5.310880829015544|       920|      706|     1027|          378|    706|\n",
      "|           Elven Bow|   Weapon|   85|     1544| 5.505181347150259|       838|      791|     1027|          463|    791|\n",
      "|     Earthquake Rune|   Weapon|   88|     1544| 5.699481865284974|       753|      879|     1027|          551|    879|\n",
      "|  Dragon Scale Armor|    Armor|   90|     1544| 5.829015544041451|       665|     1059|      278|          278|    969|\n",
      "|         Dwarven Axe|   Weapon|   90|     1544| 5.829015544041451|       665|     1059|     1027|          641|   1059|\n",
      "|     Frostbite Blade|   Weapon|   91|     1544|  5.89378238341969|       485|     1150|     1027|          732|   1150|\n",
      "|       Mithril Sword|   Weapon|   95|     1544| 6.152849740932642|       394|     1245|     1027|          827|   1245|\n",
      "|  Resurrection Stone|   Potion|   99|     1544| 6.411917098445596|       299|     1344|      239|          239|   1344|\n",
      "|           Excalibur|   Weapon|  100|     1544| 6.476683937823833|       200|     1544|     1027|         1027|   1444|\n",
      "|     Phoenix Feather|   Weapon|  100|     1544| 6.476683937823833|       200|     1544|     1027|         1027|   1544|\n",
      "+--------------------+---------+-----+---------+------------------+----------+---------+---------+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT name, item_type, avg(power) over(PARTITION BY item_type) as avg_power\n",
    "FROM fantasy_items\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT name, item_type, power, \n",
    "    sum(power) over() as sum_power, \n",
    "    power / sum(power) over() * 100 as percent_power,\n",
    "    sum(power) over(order by power desc) as desc_power,\n",
    "    sum(power) over(order by power asc) as asc_power,\n",
    "    sum(power) over(PARTITION BY item_type) as par_power,\n",
    "    sum(power) over(PARTITION BY item_type order by power asc) as par_asc_power,\n",
    "    sum(power) over(order by power asc, weight desc) as w_power\n",
    "FROM fantasy_items\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64d68cfa-28cc-4999-9a18-c61c0d72fbd0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-------+----------+----+\n",
      "|item_id|value|row_num|dense_rank|rank|\n",
      "+-------+-----+-------+----------+----+\n",
      "|     16|   30|      1|         1|   1|\n",
      "|      4|   50|      2|         2|   2|\n",
      "|      4|   50|      3|         2|   2|\n",
      "|    101|   60|      4|         3|   4|\n",
      "|      9|  150|      5|         4|   5|\n",
      "|     10|  160|      6|         5|   6|\n",
      "|      6|  200|      7|         6|   7|\n",
      "|      7|  700|      8|         7|   8|\n",
      "|      8|  800|      9|         8|   9|\n",
      "|      2| 1100|     10|         9|  10|\n",
      "|     12| 1200|     11|        10|  11|\n",
      "|      3| 1300|     12|        11|  12|\n",
      "|     13| 1300|     13|        11|  12|\n",
      "|     14| 1400|     14|        12|  14|\n",
      "|     15| 1500|     15|        13|  15|\n",
      "|     99| 1500|     16|        13|  15|\n",
      "|     11| 1700|     17|        14|  17|\n",
      "|     17| 1700|     18|        14|  17|\n",
      "|     18| 1800|     19|        15|  19|\n",
      "|    121| 2000|     20|        16|  20|\n",
      "+-------+-----+-------+----------+----+\n",
      "\n",
      "+-------+-----+----+\n",
      "|  class|level|rank|\n",
      "+-------+-----+----+\n",
      "| Archer|   26|   1|\n",
      "| Archer|   22|   2|\n",
      "| Archer|   19|   3|\n",
      "| Hobbit|   13|   1|\n",
      "| Hobbit|   12|   2|\n",
      "| Hobbit|   11|   3|\n",
      "| Hobbit|   11|   3|\n",
      "|   Mage|   40|   1|\n",
      "|   Mage|   30|   2|\n",
      "|   Mage|   28|   3|\n",
      "|   Mage|   27|   4|\n",
      "|Warrior|   25|   1|\n",
      "|Warrior|   20|   2|\n",
      "|Warrior|   18|   3|\n",
      "|Warrior|   17|   4|\n",
      "+-------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT item_id, value,\n",
    "    row_number() over(order by value) as `row_num`, \n",
    "    dense_rank() over(order by value) as `dense_rank`, \n",
    "    rank() over(order by value) as `rank`\n",
    "FROM fantasy_inventory\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT class, level,\n",
    "    rank() over(PARTITION BY class ORDER BY level DESC) as `rank`\n",
    "FROM fantasy_characters\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "176cfb97-6ac0-4215-9249-ef503f460775",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "fantasy (2)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
